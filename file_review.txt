-------------------------PROJECT STRUCTURE--------------------

uniorien-chatbot-service/
├── app
│   ├── __init__.py  # Khởi tạo Flask app
│   ├── routes.py    # API endpoints: /api/chat (xử lý query, intent, route đến agents)
│   └── utils.py     # Utility chung (nếu cần, ví dụ: format response)
├── components
│   ├── database
│   │   ├── postgres_db.py
│   │   └── chroma_db.py  # VectorDB implementation, cung cấp phương thức truy vấn vector database
│   ├── embedding
│   │   ├── ollama_embedder.py         # Optional fallback
│   │   └── sentence_transformer_embedder.py  # Chính cho embedding text
│   ├── generation
│   │   ├── ollama_generator.py  # Fallback
│   │   └── openai_generator.py  # Chính
│   ├── interfaces.py  # Abstract classes cho Generator, Embedding, VectorDatabase
│   └── manager.py     # Singleton managers cho LLM, Embedding, DB, Prompt
├── prompt
│   ├── intent_classification.txt  # Prompt phân loại ý định (SQL, RAG, kết hợp)
│   ├── sql_query.txt              # Prompt cho SQL Agent (Text-to-SQL)
│   ├── rag_query.txt              # Prompt cho RAG query
│   └── synthesis.txt              # Prompt tổng hợp kết quả từ SQL + RAG
├── service
│   ├── guardrail_service.py  # Điều chỉnh cho Intent Classification
│   ├── rag_service.py   # RAG core (ingest text từ DB, query vector)
│   ├── sql_agent_service.py  # Mới: SQL Agent với LangChain
│   └── ingestion_service.py  # Mới: Ingest text fields từ PostgreSQL vào ChromaDB
├── .dockerignore
├── .gitignore
├── Dockerfile
├── README.md
├── requirements.txt  # Thêm langchain, langchain-community, psycopg2 (cho PostgreSQL)
└── run.py            # Khởi động Flask server



----------------WORKFLOWS--------------------
a. Khởi Động Hệ Thống (run.py):

    Khởi tạo managers (GenerationManager với OpenAI chính, Ollama fallback qua env var như LLM_PROVIDER='openai').
    Kết nối PostgreSQL (qua env: DB_HOST, DB_NAME, etc.).
    Chạy ingestion ban đầu nếu cần (ingestion_service.ingest_text_from_db() để embed reviews vào ChromaDB).
    Khởi động Flask server.

b. Ingestion Ban Đầu (ingestion_service.py):

    Kết nối PostgreSQL, query các bảng có text fields (ví dụ: SELECT id, review_text FROM reviews).
    Phân chia text (RecursiveCharacterTextSplitter), embed (EmbeddingManager), lưu vào ChromaDB với metadata
        (ví dụ: {source: 'reviews', university_id: id}).
    Chạy một lần hoặc định kỳ (không watcher, có thể dùng cron job ngoài).

c. Xử Lý Query Người Dùng (routes.py: /api/chat):

    Nhận Request: Payload: { "query": "câu hỏi", "context": [{"user": "prev query", "assistant": "prev response"}, ...] }
        (context từ FE để giữ lịch sử ngắn hạn).
    Intent Classification (guardrail_service.py):
    Sử dụng LLM (GenerationManager.generate với prompt intent_classification.txt, kết hợp query + context).
    Output: "SQL" (phân tích/thống kê), "RAG" (ý kiến/review), "HYBRID" (kết hợp).
    Nếu không rõ: Trả lỗi hoặc default RAG.

Thực Thi Agent:
    Nếu SQL: Gọi sql_agent_service.execute(query + context) → Sử dụng LangChain's create_sql_agent
        (với SQLDatabase.from_uri cho PostgreSQL, prompt sql_query.txt)
            → Tạo và chạy SQL query → Lấy kết quả bảng (ví dụ: danh sách trường, điểm chuẩn).
    Nếu RAG: Gọi mini_rag_service.query(query + context)
            → Embed query → Query ChromaDB → Lấy chunks liên quan (reviews/mô tả).
    Nếu HYBRID: Chạy SQL trước → Lấy kết quả (ví dụ: danh sách trường lọc theo điểm)
        → Sử dụng kết quả làm filter cho RAG query (ví dụ: query vector với where_filter={university_id: in list}) → Lấy reviews.

Tổng Hợp (GenerationManager):
    LLM nhận kết quả từ SQL/RAG (hoặc cả hai), kết hợp với context, sử dụng prompt synthesis.txt để tạo response cuối cùng
        (hoàn chỉnh, tiếng Việt).

    Trả Response: { "answer": "response", "sources": ["SQL: table X", "RAG: chunk Y"] } (nếu cần trace).

d. Error Handling và Optimization:

    SQL Agent: Sử dụng agent executor với handle_parsing_errors=True để tự sửa query sai.
    RAG: Giới hạn n_results=5 để tránh overload.
    LLM Fallback: Nếu OpenAI lỗi (rate limit), switch sang Ollama.
    Security: Validate query để tránh SQL injection (LangChain xử lý phần nào).
    Scaling: Sử dụng async cho API nếu query DB chậm.



----------------interfaces.py----------------

from __future__ import annotations
from abc import ABC, abstractmethod
from typing import List, Dict, Optional, Any


# -----------------------------
# 1. Generator Interface (LLM)
# -----------------------------
class IGenerator(ABC):
    """
    Interface cho tất cả LLM providers (OpenAI, Ollama, v.v.)
    """

    @abstractmethod
    def generate(
        self,
        system_prompt: str,
        user_prompt: str,
        context: Optional[List[Dict[str, str]]] = None,
        model: Optional[str] = None,
        **kwargs: Any,
    ) -> str:
        """
        Sinh output hoàn chỉnh từ LLM.
        Return: final text response từ LLM.
        """
        pass


# --------------------------------
# 2. Embedding Interface
# --------------------------------
class IEmbedder(ABC):
    """
    Interface embedding providers (SentenceTransformer, OpenAI embedding, v.v.)
    """

    @abstractmethod
    def embed(self, text: str) -> List[float]:
        """
        Embed một đoạn text thành vector.
        """
        pass

    @abstractmethod
    def embed_batch(self, texts: List[str]) -> List[List[float]]:
        """
        Embed nhiều text cùng lúc (tối ưu cho ingestion).
        """
        pass


# --------------------------------
# 3. Vector Database Interface
# --------------------------------
class IVectorDatabase(ABC):
    """
    Interface cho Vector DB (ChromaDB hiện tại).
    """

    @abstractmethod
    def add_documents(
        self,
        texts: List[str],
        embeddings: List[List[float]],
        metadatas: Optional[List[Dict[str, Any]]] = None,
        ids: Optional[List[str]] = None,
    ) -> None:
        """
        Thêm documents vào DB.
        """
        pass

    @abstractmethod
    def query(
        self,
        embedding: List[float],
        n_results: int = 5,
        where: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        Query gần giống nhất.
        Return: dict chứa texts, ids, metadatas,...
        """
        pass

    @abstractmethod
    def delete(self, ids: List[str]) -> None:
        """
        Xoá documents theo ID.
        """
        pass


class ISQLDatabase(ABC):
    """
    Abstraction layer for SQL database access used by AI service.
    Implementation examples: PostgresDatabase, MySQLDatabase, SQLiteDatabase.
    """

    @abstractmethod
    def run(self, query: str) -> Any:
        """Execute SQL query and return result (DataFrame or list[dict])."""
        pass

    @abstractmethod
    def get_table_info(self, table_names: Optional[List[str]] = None) -> str:
        """Return table schema used for SQL Agent."""
        pass


----------------manager.py----------------

from __future__ import annotations
import os
from typing import Optional, Dict

from components.interfaces import ISQLDatabase
from interfaces import IGenerator, IEmbedder, IVectorDatabase


class GenerationManager:
    """
    Quản lý các LLM providers: OpenAI (main), Ollama (fallback).
    """

    _instance: Optional["GenerationManager"] = None

    def __init__(self):
        self.primary: Optional[IGenerator] = None
        self.fallback: Optional[IGenerator] = None

    @classmethod
    def instance(cls) -> "GenerationManager":
        if cls._instance is None:
            cls._instance = GenerationManager()
        return cls._instance

    def configure(self, primary: IGenerator, fallback: Optional[IGenerator] = None):
        self.primary = primary
        self.fallback = fallback

    def generate(self, **kwargs) -> str:
        if self.primary is None:
            raise RuntimeError("GenerationManager is not configured with primary generator.")

        try:
            return self.primary.generate(**kwargs)
        except Exception as e:
            if self.fallback:
                print(f"[WARN] Primary LLM failed ({type(e).__name__}), switching to fallback...")
                return self.fallback.generate(**kwargs)
            raise RuntimeError(f"Primary LLM failed and no fallback available: {e}")


class EmbeddingManager:
    """
    Quản lý embedder (SentenceTransformer).
    """

    _instance: Optional["EmbeddingManager"] = None

    def __init__(self):
        self.fallback = None
        self.primary = None
        self.embedder: Optional[IEmbedder] = None

    @classmethod
    def instance(cls) -> "EmbeddingManager":
        if cls._instance is None:
            cls._instance = EmbeddingManager()
        return cls._instance

    def configure(self, embedder: IEmbedder):
        self.embedder = embedder

    def embed(self, text: str):
        if not self.embedder:
            raise RuntimeError("EmbeddingManager is not configured.")
        return self.embedder.embed(text)

    def embed_batch(self, texts: list[str]):
        if not self.embedder:
            raise RuntimeError("EmbeddingManager is not configured.")
        return self.embedder.embed_batch(texts)


class VectorDatabaseManager:
    """
    Quản lý vector database (ChromaDB).
    """

    _instance: Optional["VectorDatabaseManager"] = None

    def __init__(self):
        self.db: Optional[IVectorDatabase] = None

    @classmethod
    def instance(cls) -> "VectorDatabaseManager":
        if cls._instance is None:
            cls._instance = VectorDatabaseManager()
        return cls._instance

    def configure(self, db: IVectorDatabase):
        self.db = db

    def add_documents(self, **kwargs):
        if not self.db:
            raise RuntimeError("VectorDBManager is not configured.")
        return self.db.add_documents(**kwargs)

    def query(self, **kwargs):
        if not self.db:
            raise RuntimeError("VectorDBManager is not configured.")
        return self.db.query(**kwargs)

    def delete(self, ids: list[str]):
        if not self.db:
            raise RuntimeError("VectorDBManager is not configured.")
        return self.db.delete(ids)


class PromptManager:
    """
    Tải và cache prompt .txt từ thư mục /prompt.
    """

    _instance: Optional["PromptManager"] = None

    def __init__(self):
        self.cache: Dict[str, str] = {}
        self.base_path = os.path.join(os.getcwd(), "prompt")
        # self.base_path = os.path.join(os.path.dirname(__file__), "..", "prompt")

    @classmethod
    def instance(cls) -> "PromptManager":
        if cls._instance is None:
            cls._instance = PromptManager()
        return cls._instance

    def load(self, name: str) -> str:
        """
        name: tên file không kèm path (vd: 'intent_classification.txt')
        """
        if name in self.cache:
            return self.cache[name]

        file_path = os.path.join(self.base_path, name)
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Prompt file not found: {file_path}")

        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read().strip()

        self.cache[name] = content
        return content


class SQLDatabaseManager:
    """
    Singleton manager for the SQL database instance.

    Responsibilities:
    - Hold a single instance of ISQLDatabase.
    - Ensure the database is configured before being used.
    - Provide global access point for SQL-related operations in AI service.
    """

    _instance: Optional["SQLDatabaseManager"] = None

    def __init__(self):
        self.db: Optional[ISQLDatabase] = None

    @classmethod
    def instance(cls) -> "SQLDatabaseManager":
        if cls._instance is None:
            cls._instance = SQLDatabaseManager()
        return cls._instance

    def configure(self, db: ISQLDatabase) -> None:
        """
        Configure the database. Must be called once during app startup.
        """
        self.db = db

    def is_configured(self) -> bool:
        return self.db is not None

    def get_db(self) -> ISQLDatabase:
        """
        Return the configured database instance.
        Raise detailed error if database is not ready.
        """
        if not self.db:
            raise RuntimeError(
                "SQLDatabaseManager is not configured. "
                "Call SQLDatabaseManager.instance().configure(PostgresDatabase(...)) first."
            )
        return self.db


----------------openai_generator.py----------------

import os
import openai
from typing import List, Dict, Optional

from components.interfaces import IGenerator


class OpenAIGenerator(IGenerator):
    """
    Primary LLM Provider: OpenAI (gpt-4o-mini / gpt-4o)
    """

    def __init__(self, model: str = "gpt-4o-mini"):
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise RuntimeError("OPENAI_API_KEY is missing.")

        openai.api_key = api_key
        self.model = model

    def generate(
        self,
        system_prompt: str,
        user_prompt: str,
        context: Optional[List[Dict[str, str]]] = None,
        temperature: float = 0.3,
        max_tokens: int = 1024,
    ) -> str:
        """
        Generate response từ OpenAI Chat Completion.
        """

        messages = [{"role": "system", "content": system_prompt}]

        if context:
            messages.extend(context)

        messages.append({"role": "user", "content": user_prompt})

        try:
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            return response.choices[0].message["content"]

        except Exception as e:
            raise RuntimeError(f"OpenAI generation failed: {str(e)}")


----------------ollama_generator.py----------------

import requests
from typing import List, Dict, Optional

from components.interfaces import IGenerator


class OllamaGenerator(IGenerator):
    """
    Fallback LLM provider, using Ollama server.
    """

    def __init__(self,
                 model: str = "llama3.1:8b",
                 host: str = "http://localhost:11434"):
        self.model = model
        self.host = host.rstrip("/")

    def generate(
        self,
        system_prompt: str,
        user_prompt: str,
        context: Optional[List[Dict[str, str]]] = None,
        temperature: float = 0.4,
        max_tokens: int = 1024,
    ) -> str:

        messages = [{"role": "system", "content": system_prompt}]

        if context:
            messages.extend(context)

        messages.append({"role": "user", "content": user_prompt})

        payload = {
            "model": self.model,
            "messages": messages,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            },
        }

        try:
            response = requests.post(
                f"{self.host}/api/chat",
                json=payload,
                timeout=30,
            )
            response.raise_for_status()
            data = response.json()

            # Ollama trả nội dung trong data["message"]["content"]
            return data.get("message", {}).get("content", "")

        except Exception as e:
            raise RuntimeError(f"Ollama generation failed: {str(e)}")


----------------sentence_transformer_embedder.py----------------

# components/embedding/sentence_transformer_embedder.py
from __future__ import annotations
from typing import List

from sentence_transformers import SentenceTransformer
from components.interfaces import IEmbedder


class SentenceTransformerEmbedder(IEmbedder):

    def __init__(self, model_name: str = "AITeamVN/Vietnamese_Embedding"):
        # Tự động dùng GPU nếu có, không cần config phức tạp
        self.model = SentenceTransformer(
            model_name, device="cuda" if SentenceTransformer.is_cuda_available() else "cpu")

    def embed(self, text: str) -> List[float]:
        if not text or not text.strip():
            return []
        # normalize_embeddings=True → cosine similarity chính xác hơn trong Chroma
        vec = self.model.encode([text.strip()], normalize_embeddings=True, convert_to_numpy=True)
        return vec[0].tolist()

    def embed_batch(self, texts: List[str]) -> List[List[float]]:
        if not texts:
            return []

        # Lọc text rỗng để tránh warning từ SentenceTransformer
        cleaned_texts = [t.strip() for t in texts if t and t.strip()]
        if not cleaned_texts:
            return [[] for _ in texts]

        embeddings = self.model.encode(
            cleaned_texts,
            normalize_embeddings=True,    # Quan trọng!
            convert_to_numpy=True,
            show_progress_bar=False,
            batch_size=32,  # cố định hợp lý, không cần env
        )
        # Giữ đúng thứ tự gốc
        result = []
        cleaned_idx = 0
        for t in texts:
            if t and t.strip():
                result.append(embeddings[cleaned_idx].tolist())
                cleaned_idx += 1
            else:
                result.append([])
        return result

----------------ollama_embedder.py----------------

from __future__ import annotations
from typing import List
import requests
from components.interfaces import IEmbedder


class OllamaEmbedder(IEmbedder):
    """
    Fallback embedder — sử dụng Ollama local embedding model.
    Hỗ trợ cả embed (single) và embed_batch (batch).
    """

    def __init__(self, model_name: str = "nomic-embed-text",
                 host: str = "http://localhost:11434"):
        self.model_name = model_name
        self.host = host.rstrip("/")

    def embed(self, text: str) -> List[float]:
        """
        Embed một chuỗi đơn lẻ.
        """
        if text is None or text == "":
            return []

        url = f"{self.host}/api/embeddings"
        payload = {"model": self.model_name, "input": text}
        resp = requests.post(url, json=payload, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        # Kỳ vọng: {"embeddings": [ [float], ... ]} hoặc {"embedding": [...]}
        if "embeddings" in data:
            return data["embeddings"][0]
        if "embedding" in data:
            return data["embedding"]
        # Fallback: empty vector
        return []

    def embed_batch(self, texts: List[str]) -> List[List[float]]:
        """
        Embed nhiều đoạn text cùng lúc.
        """
        if not texts:
            return []

        url = f"{self.host}/api/embeddings"
        payload = {"model": self.model_name, "input": texts}
        resp = requests.post(url, json=payload, timeout=60)
        resp.raise_for_status()
        data = resp.json()
        # Kỳ vọng: {"embeddings": [ [float], ... ]}
        if "embeddings" in data:
            return data["embeddings"]
        # Nếu API trả embedding cho mỗi input dưới key khác, attempt to normalize:
        if isinstance(data, dict):
            # try to find a top-level list of lists
            for v in data.values():
                if isinstance(v, list) and v and isinstance(v[0], list):
                    return v  # best-effort
        # Fallback: return empty list
        return []


----------------chroma_db.py----------------

from __future__ import annotations
from typing import List, Dict, Any, Optional
from components.interfaces import IVectorDatabase

import chromadb
from chromadb.config import Settings


class ChromaDB(IVectorDatabase):

    def __init__(
        self,
        persist_dir: str = "./data/chroma",
        collection_name: str = "uniorien_collection",
    ):
        self.persist_dir = persist_dir
        self.collection_name = collection_name

        # Tạo client
        self.client = chromadb.Client(
            Settings(
                chroma_db_impl="duckdb+parquet",
                persist_directory=persist_dir,
            )
        )

        # Lấy hoặc tạo collection (không cung cấp embedding_function)
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"},
        )


    def add_documents(
        self,
        texts: List[str],
        embeddings: List[List[float]],
        metadatas: Optional[List[Dict[str, Any]]] = None,
        ids: Optional[List[str]] = None,
    ) -> None:

        if ids is None:
            ids = [str(i) for i in range(len(texts))]

        # Chroma yêu cầu mọi list phải cùng số lượng
        if len(texts) != len(embeddings):
            raise ValueError("texts và embeddings phải có cùng độ dài.")

        if metadatas and len(metadatas) != len(texts):
            raise ValueError("metadatas phải cùng độ dài với texts.")

        self.collection.add(
            ids=ids,
            documents=texts,
            embeddings=embeddings,
            metadatas=metadatas,
        )


    def query(
        self,
        embedding: List[float],
        n_results: int = 5,
        where: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:

        if n_results < 1:
            n_results = 1
        if n_results > 100:
            n_results = 100

        results = self.collection.query(
            query_embeddings=[embedding],
            n_results=n_results,
            where=where,
            include=["documents", "embeddings", "metadatas", "distances", "ids"],
        )

        # Chroma trả về mảng 2D — ta lấy hàng đầu tiên
        return {
            "ids": results.get("ids", [[]])[0],
            "documents": results.get("documents", [[]])[0],
            "metadatas": results.get("metadatas", [[]])[0],
            "distances": results.get("distances", [[]])[0],
        }

    def delete(self, ids: List[str]) -> None:
        self.collection.delete(ids=ids)


----------------postgres_db.py----------------

from __future__ import annotations
from typing import Optional, List, Any
from langchain_community.utilities import SQLDatabase
from components.interfaces import ISQLDatabase


class PostgresDatabase(ISQLDatabase):
    """
    Concrete implementation of ISQLDatabase using LangChain SQLDatabase wrapper.
    """

    def __init__(self, db_uri: str, include_tables: Optional[List[str]] = None):
        self.db = SQLDatabase.from_uri(
            db_uri,
            include_tables=include_tables,      # Whitelist bảng được phép truy cập
            sample_rows_in_table_info=3,
        )

    def run(self, query: str) -> Any:
        """
        Execute SQL query.
        LangChain SQLDatabase.run() may return a string, so we normalize it.
        """
        try:
            raw = self.db.run(query)
            return self._normalize_result(raw)
        except Exception as e:
            raise RuntimeError(f"Database query failed: {str(e)}")

    def get_table_info(self, table_names: Optional[List[str]] = None) -> str:
        """
        Schema information used by SQL Agent.
        """
        try:
            return self.db.get_table_info(table_names)
        except Exception as e:
            raise RuntimeError(f"Failed to load table info: {str(e)}")

    def _normalize_result(self, raw: Any) -> Any:
        """
        Convert raw response from LangChain SQLDatabase to a cleaner format.
        If LangChain returns a string table, keep it (SQL Agent expects this).
        """
        if isinstance(raw, str):
            return raw

        # Nếu sau này bạn muốn parse thành DataFrame hoặc list[dict], làm tại đây.
        return raw



----------------ingestion_service.py----------------

from __future__ import annotations
from typing import List, Dict, Any, Optional

import os
import logging

from langchain_text_splitters import RecursiveCharacterTextSplitter

from components.manager import EmbeddingManager, VectorDatabaseManager
from components.interfaces import IVectorDatabase, IEmbedder


logger = logging.getLogger(__name__)


class IngestionService:
    """
    Dịch vụ ingestion cho UniOrien AI.
    Nhiệm vụ:
        - Load raw text từ file
        - Chunk
        - Embed batch
        - Lưu vào VectorDB (Chroma)
    """

    def __init__(
        self,
        chunk_size: int = 800,  # lớn hơn một chút để giảm số chunk
        chunk_overlap: int = 200,
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", " ", ""],
        )

        # Lấy managers
        self.embedder: IEmbedder = EmbeddingManager.instance().primary or EmbeddingManager.instance().fallback
        if not self.embedder:
            raise RuntimeError("EmbeddingManager is not configured")

        self.vector_db: IVectorDatabase = VectorDatabaseManager.instance().db
        if not self.vector_db:
            raise RuntimeError("VectorDBManager is not configured")

    # ---------------------------------------------------------------------
    # FILE LOADING
    # ---------------------------------------------------------------------
    def load_text_from_file(self, file_path: str) -> str:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")

        _, ext = os.path.splitext(file_path)
        ext = ext.lower()

        if ext == ".txt":
            with open(file_path, "r", encoding="utf-8") as f:
                return f.read()

        raise ValueError(f"File format is not supported: {ext}")

    # ---------------------------------------------------------------------
    # INGEST SINGLE FILE
    # ---------------------------------------------------------------------
    def ingest_file(
        self,
        file_path: str,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        Ingest một file vào Vector DB.
        Return:
            {
                "num_chunks": ...,
                "file": file_path
            }
        """
        logger.info(f"[INGEST] Ingesting file: {file_path}")

        raw_text = self.load_text_from_file(file_path)
        chunks = self.text_splitter.split_text(raw_text)

        logger.info(f"[INGEST] Total chunks were generate: {len(chunks)}")

        # Tạo metadata cho mỗi chunk
        metadatas = []
        for idx, _ in enumerate(chunks):
            md = {
                "source": file_path,
                "chunk_index": idx,
            }
            if metadata:
                md.update(metadata)
            metadatas.append(md)

        # Embedding batch
        embeddings = self.embedder.embed_batch(chunks)
        if not embeddings:
            raise RuntimeError("Embedding batch is empty.")

        # IDs
        ids = [f"{os.path.basename(file_path)}_{i}" for i in range(len(chunks))]

        # Lưu vào Vector DB
        self.vector_db.add_documents(
            texts=chunks,
            embeddings=embeddings,
            metadatas=metadatas,
            ids=ids,
        )

        logger.info(f"[INGEST] Completed ingest file: {file_path}")

        return {
            "file": file_path,
            "num_chunks": len(chunks),
        }

    # ---------------------------------------------------------------------
    # INGEST MULTIPLE FILES
    # ---------------------------------------------------------------------
    def ingest_files(
        self,
        file_paths: List[str],
        common_metadata: Optional[Dict[str, Any]] = None,
    ) -> List[Dict[str, Any]]:
        results = []
        for path in file_paths:
            r = self.ingest_file(path, metadata=common_metadata)
            results.append(r)
        return results





